{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pyshark\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from numpy import zeros\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import argparse\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T22:03:50.563195Z",
     "start_time": "2024-11-20T22:03:49.275524Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# argparser = argparse.ArgumentParser()\n",
    "# #''' Switch between the toy and REAL EXAMPLES\n",
    "# argparser.add_argument(\"--lang1\", help=\"Language 1 class\",\n",
    "#                        type=str, default=\"./data/englishpcaps\")\n",
    "# argparser.add_argument(\"--lang2\", help=\"Language 2 class\",\n",
    "#                        type=str, default=\"./data/spanishpcaps\")\n",
    "# argparser.add_argument(\"--passes\", help=\"Number of passes through train\",\n",
    "#                        type=int, default=5)\n",
    "# argparser.add_argument(\"--batch\", help=\"Number of items in each batch\",\n",
    "#                        type=int, default=1)\n",
    "# argparser.add_argument(\"--learnrate\", help=\"Learning rate for SGD\",\n",
    "#                        type=float, default=0.1)\n",
    "\n",
    "# args = argparser.parse_args()\n",
    "args = {\"lang1\":'./data/smallenglish', \"lang2\":'./data/smallspanish', \"passes\":5, \"batch\":1, \"learnrate\":0.1}\n",
    "\n",
    "vocab = [[0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 2], [0, 0, 1, 0], [0, 0, 1, 1], [0, 0, 1, 2],\n",
    " [0, 0, 2, 0], [0, 0, 2, 1], [0, 0, 2, 2], [0, 1, 0, 0], [0, 1, 0, 1], [0, 1, 0, 2],\n",
    " [0, 1, 1, 0], [0, 1, 1, 1], [0, 1, 1, 2], [0, 1, 2, 0], [0, 1, 2, 1], [0, 1, 2, 2],\n",
    " [0, 2, 0, 0], [0, 2, 0, 1], [0, 2, 0, 2], [0, 2, 1, 0], [0, 2, 1, 1], [0, 2, 1, 2],\n",
    " [0, 2, 2, 0], [0, 2, 2, 1], [0, 2, 2, 2], [1, 0, 0, 0], [1, 0, 0, 1], [1, 0, 0, 2],\n",
    " [1, 0, 1, 0], [1, 0, 1, 1], [1, 0, 1, 2], [1, 0, 2, 0], [1, 0, 2, 1], [1, 0, 2, 2],\n",
    " [1, 1, 0, 0], [1, 1, 0, 1], [1, 1, 0, 2], [1, 1, 1, 0], [1, 1, 1, 1], [1, 1, 1, 2],\n",
    " [1, 1, 2, 0], [1, 1, 2, 1], [1, 1, 2, 2], [1, 2, 0, 0], [1, 2, 0, 1], [1, 2, 0, 2],\n",
    " [1, 2, 1, 0], [1, 2, 1, 1], [1, 2, 1, 2], [1, 2, 2, 0], [1, 2, 2, 1], [1, 2, 2, 2],\n",
    " [2, 0, 0, 0], [2, 0, 0, 1], [2, 0, 0, 2], [2, 0, 1, 0], [2, 0, 1, 1], [2, 0, 1, 2],\n",
    " [2, 0, 2, 0], [2, 0, 2, 1], [2, 0, 2, 2], [2, 1, 0, 0], [2, 1, 0, 1], [2, 1, 0, 2],\n",
    " [2, 1, 1, 0], [2, 1, 1, 1], [2, 1, 1, 2], [2, 1, 2, 0], [2, 1, 2, 1], [2, 1, 2, 2],\n",
    " [2, 2, 0, 0], [2, 2, 0, 1], [2, 2, 0, 2], [2, 2, 1, 0], [2, 2, 1, 1], [2, 2, 1, 2],\n",
    " [2, 2, 2, 0], [2, 2, 2, 1], [2, 2, 2, 2]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T22:03:53.338036Z",
     "start_time": "2024-11-20T22:03:53.326363Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def pcap_to_lengths(pcap_file) -> [int]:\n",
    "    length = []\n",
    "    capture = pyshark.FileCapture(pcap_file)\n",
    "    for packet in capture:\n",
    "        length.append(packet.length)\n",
    "\n",
    "    return length\n",
    "\n",
    "def lengths_to_tokens(lengths: [int]) -> [int]:\n",
    "    # 0 = 2 smallest\n",
    "    # 2 = largest length\n",
    "    # 1 = everything else\n",
    "\n",
    "    lengths_mod = lengths\n",
    "    lengths_mod.remove(min(lengths_mod))\n",
    "\n",
    "    min_length2 = min(lengths_mod) # second smallest\n",
    "    min_length = min(lengths)      # smallest\n",
    "    max_length = max(lengths)      # largest\n",
    "\n",
    "    for i in range(len(lengths)):\n",
    "        if lengths[i] == min_length or lengths[i] == min_length2:\n",
    "            lengths[i] = 0\n",
    "        elif lengths[i] == max_length:\n",
    "            lengths[i] = 2\n",
    "        else:\n",
    "            lengths[i] = 1\n",
    "\n",
    "    return lengths\n",
    "\n",
    "def tokens_to_tuples(tokens: [int]) -> [(int, int, int, int)]:\n",
    "    tuples = []\n",
    "    i = 0\n",
    "    while i + 3 < len(tokens):\n",
    "        tuples.append((tokens[i], tokens[i+1], tokens[i+2], tokens[i+3]))\n",
    "        i = i+1\n",
    "\n",
    "    return tuples\n",
    "\n",
    "def count_tuples(words: [(int, int, int, int)]) -> [int]:\n",
    "    # return type is {int: int} where key is index of tuple in vocab and value is the count\n",
    "    count = []\n",
    "    for i in range(len(vocab)):\n",
    "        count.append(words.count(vocab[i]))\n",
    "\n",
    "    return count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T22:03:54.968163Z",
     "start_time": "2024-11-20T22:03:54.960368Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "read dataset \n",
    "\"\"\"\n",
    "    :param lang1: directory names of pcap files of lang1 (string)\n",
    "    :param lang2: directory names of pcap files of lang2 (string)\n",
    "\n",
    "    read in files and create a list of pcap files of each language\n",
    "\n",
    "    turn each pcap file into a list of individual packet lengths\n",
    "    identify the lengths that each token represents\n",
    "    turn each list of packet lengths into a list of tokens\n",
    "    create a list of 4-tuples of tokens in each pcap file.\n",
    "\n",
    "    create lengths1 and lengths2\n",
    "    length1: list of lists of lengths\n",
    "\n",
    "    tokens =\n",
    "    [\n",
    "    0 token representing two smallest lengths,\n",
    "    1 token representing middle lengths,\n",
    "    2 token representing largest length\n",
    "    ]\n",
    "\n",
    "    vocab =\n",
    "    [\n",
    "    (0, 0, 0, 0), (0, 0, 0, 1), (0, 0, 1, 1), ...]\n",
    "\n",
    "    [m, s, s, s, m, l, m, s, m, m]\n",
    "    (m, s, s, s), (s, s, s, m), (s, s, m, l), (s, m, l, m)\n",
    "\n",
    "    matrix will be a 2D array where each row is a sample file and columns are class(language), the 81 columns\n",
    "    of the count in vocab\n",
    "\n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m pcap_files2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(directory2\u001B[38;5;241m.\u001B[39mglob(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m*.pcapng\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# convert pcaps to lengths\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m lang1_lengths \u001B[38;5;241m=\u001B[39m [pcap_to_lengths(f) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m pcap_files1]\n\u001B[1;32m     11\u001B[0m lang2_lengths \u001B[38;5;241m=\u001B[39m [pcap_to_lengths(f) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m pcap_files2]\n",
      "Cell \u001B[0;32mIn[4], line 10\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      8\u001B[0m pcap_files2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(directory2\u001B[38;5;241m.\u001B[39mglob(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m*.pcapng\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# convert pcaps to lengths\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m lang1_lengths \u001B[38;5;241m=\u001B[39m [\u001B[43mpcap_to_lengths\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m pcap_files1]\n\u001B[1;32m     11\u001B[0m lang2_lengths \u001B[38;5;241m=\u001B[39m [pcap_to_lengths(f) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m pcap_files2]\n",
      "Cell \u001B[0;32mIn[3], line 4\u001B[0m, in \u001B[0;36mpcap_to_lengths\u001B[0;34m(pcap_file)\u001B[0m\n\u001B[1;32m      2\u001B[0m length \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      3\u001B[0m capture \u001B[38;5;241m=\u001B[39m pyshark\u001B[38;5;241m.\u001B[39mFileCapture(pcap_file)\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m packet \u001B[38;5;129;01min\u001B[39;00m capture:\n\u001B[1;32m      5\u001B[0m     length\u001B[38;5;241m.\u001B[39mappend(packet\u001B[38;5;241m.\u001B[39mlength)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m length\n",
      "File \u001B[0;32m~/Documents/college/f24/723/cl1-hw/lr_pytorch/venv/lib/python3.9/site-packages/pyshark/capture/capture.py:212\u001B[0m, in \u001B[0;36mCapture._packets_from_tshark_sync\u001B[0;34m(self, packet_count, existing_process)\u001B[0m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Returns a generator of packets.\u001B[39;00m\n\u001B[1;32m    205\u001B[0m \n\u001B[1;32m    206\u001B[0m \u001B[38;5;124;03mThis is the sync version of packets_from_tshark. It wait for the completion of each coroutine and\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    209\u001B[0m \u001B[38;5;124;03m:param packet_count: If given, stops after this amount of packets is captured.\u001B[39;00m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    211\u001B[0m \u001B[38;5;66;03m# NOTE: This has code duplication with the async version, think about how to solve this\u001B[39;00m\n\u001B[0;32m--> 212\u001B[0m tshark_process \u001B[38;5;241m=\u001B[39m existing_process \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meventloop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_until_complete\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    213\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_tshark_process\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    214\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_setup_tshark_output_parser()\n\u001B[1;32m    215\u001B[0m packets_captured \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:618\u001B[0m, in \u001B[0;36mBaseEventLoop.run_until_complete\u001B[0;34m(self, future)\u001B[0m\n\u001B[1;32m    607\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Run until the Future is done.\u001B[39;00m\n\u001B[1;32m    608\u001B[0m \n\u001B[1;32m    609\u001B[0m \u001B[38;5;124;03mIf the argument is a coroutine, it is wrapped in a Task.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    615\u001B[0m \u001B[38;5;124;03mReturn the Future's result, or raise its exception.\u001B[39;00m\n\u001B[1;32m    616\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    617\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_closed()\n\u001B[0;32m--> 618\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_running\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    620\u001B[0m new_task \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m futures\u001B[38;5;241m.\u001B[39misfuture(future)\n\u001B[1;32m    621\u001B[0m future \u001B[38;5;241m=\u001B[39m tasks\u001B[38;5;241m.\u001B[39mensure_future(future, loop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py:578\u001B[0m, in \u001B[0;36mBaseEventLoop._check_running\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    576\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_running\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    577\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_running():\n\u001B[0;32m--> 578\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThis event loop is already running\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    579\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m events\u001B[38;5;241m.\u001B[39m_get_running_loop() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    580\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    581\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCannot run the event loop while another loop is running\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "directory1 = Path(args[\"lang1\"])\n",
    "directory2 = Path(args[\"lang2\"])\n",
    "\n",
    "\n",
    "# read in every .pcap file in directories lang1 and lang2 into a list\n",
    "\n",
    "pcap_files1 = list(directory1.glob('*.pcapng'))\n",
    "pcap_files2 = list(directory2.glob('*.pcapng'))\n",
    "# convert pcaps to lengths\n",
    "lang1_lengths = [pcap_to_lengths(f) for f in pcap_files1]\n",
    "lang2_lengths = [pcap_to_lengths(f) for f in pcap_files2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T22:04:01.534501Z",
     "start_time": "2024-11-20T22:04:01.242882Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ast.py:265: RuntimeWarning: coroutine 'Capture._get_tshark_process' was never awaited\n",
      "  for item in field:\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lang1_lengths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# convert lengths to tokens\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m lang1_tokens \u001B[38;5;241m=\u001B[39m [lengths_to_tokens(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m \u001B[43mlang1_lengths\u001B[49m]\n\u001B[1;32m      3\u001B[0m lang2_tokens \u001B[38;5;241m=\u001B[39m [lengths_to_tokens(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lang2_lengths]\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# convert tokens to tuples\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'lang1_lengths' is not defined"
     ]
    }
   ],
   "source": [
    "# convert lengths to tokens\n",
    "lang1_tokens = [lengths_to_tokens(l) for l in lang1_lengths]\n",
    "lang2_tokens = [lengths_to_tokens(l) for l in lang2_lengths]\n",
    "\n",
    "# convert tokens to tuples\n",
    "lang1_tuples = [tokens_to_tuples(l) for l in lang1_tokens]\n",
    "lang2_tuples = [tokens_to_tuples(l) for l in lang2_tokens]\n",
    "\n",
    "# convert tuples to counts of each sample to insert into matrix -> [{int: int}]\n",
    "lang1_counts = [count_tuples(t) for t in lang1_tuples]\n",
    "lang2_counts = [count_tuples(t) for t in lang2_tuples]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T22:03:10.984336Z",
     "start_time": "2024-11-20T22:03:10.963278Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "final = zeros((len(lang1_counts) + len(lang2_counts), (len(vocab) + 1)))\n",
    "\n",
    "final[:len(lang1_counts), 1:] = np.asmatrix(lang1_counts)\n",
    "final[:len(lang1_counts), 1] = 0\n",
    "\n",
    "final[len(lang1_counts):, 1:] = lang2_counts\n",
    "final[len(lang1_counts):, 1] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    final = zeros((len(lang1_counts) + len(lang2_counts), (len(vocab) + 1)))\n",
    "\n",
    "    final[:len(lang1_counts), 1:] = lang1_counts\n",
    "    final[:len(lang1_counts), 1] = 0\n",
    "\n",
    "    final[len(lang1_counts):, 1:] = lang2_counts\n",
    "    final[len(lang1_counts):, 1] = 1\n",
    "\n",
    "    lang1_m = np.asmatrix(lang1_counts)\n",
    "    lang2_m = np.asmatrix(lang2_counts)\n",
    "\n",
    "\n",
    "    # create the matrix\n",
    "    for i in range(len(vocab)):\n",
    "        sample_count1 = [0] * (len(vocab) + 1)\n",
    "        sample_count2 = [0] * (len(vocab) + 1)\n",
    "        # class = 0 for lang1, class = 1 for lang2\n",
    "        sample_count2[0] = 1\n",
    "\n",
    "        sample_count1[i + 1] = lang1_counts[i]\n",
    "        sample_count2[i + 1] = lang2_counts[i]\n",
    "\n",
    "        # add both arrays to the matrix\n",
    "        matrix.append(sample_count1)\n",
    "        matrix.append(sample_count2)\n",
    "\n",
    "    langdata = matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class LangDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.n_samples, self.n_features = data.shape\n",
    "        # The first column is label, the rest are the features\n",
    "        self.n_features -= 1\n",
    "\n",
    "        assert (self.n_samples, self.n_features) == (240, 81)\n",
    "        self.feature = torch.from_numpy(data[:, 1:].astype(np.float32)) # size [n_samples, n_features]\n",
    "        self.label = torch.from_numpy(data[:, [0]].astype(np.float32)) # size [n_samples, 1]\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.feature[index], self.label[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "\n",
    "class SimpleLogreg(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        \"\"\"\n",
    "        Initialize the parameters you'll need for the model.\n",
    "\n",
    "        :param num_features: The number of features in the linear model\n",
    "        \"\"\"\n",
    "        super(SimpleLogreg, self).__init__()\n",
    "        self.linear = nn.Linear(num_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the model prediction for an example.\n",
    "\n",
    "        :param x: Example to evaluate\n",
    "        \"\"\"\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "    def evaluate(self, data):\n",
    "        with torch.no_grad():\n",
    "            y_predicted = self(data.feature)\n",
    "            y_predicted_cls = y_predicted.round()\n",
    "            acc = y_predicted_cls.eq(data.label).sum() / float(data.label.shape[0])\n",
    "            return acc\n",
    "\n",
    "    def inspect(self, vocab, limit=10):\n",
    "        \"\"\"\n",
    "        A fundtion to find the top features and print them.\n",
    "        \"\"\"\n",
    "\n",
    "        None\n",
    "        weights = logreg.linear.weight[0].detach().numpy()\n",
    "\n",
    "\n",
    "def step(epoch, ex, model, optimizer, criterion, inputs, labels):\n",
    "    \"\"\"Take a single step of the optimizer, we factored it into a single\n",
    "    function so we could write tests.\n",
    "\n",
    "\n",
    "    :param epoch: The current epoch\n",
    "    :param ex: Which example / minibatch you're one\n",
    "    :param model: The model you're optimizing\n",
    "    :param inputs: The current set of inputs\n",
    "    :param labels: The labels for those inputs\n",
    "\n",
    "    A) get predictions\n",
    "    B) compute the loss from that prediction\n",
    "    C) backprop\n",
    "    D) update the parameters\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    prediction = model(inputs)\n",
    "    loss = criterion(prediction, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (ex+1) % 20 == 0:\n",
    "      acc_train = model.evaluate(train)\n",
    "      acc_test = model.evaluate(test)\n",
    "      print(f'Epoch: {epoch+1}/{num_epochs}, Example {ex}, loss = {loss.item():.4f}, train_acc = {acc_train.item():.4f} test_acc = {acc_test.item():.4f}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "langdata = read_dataset(args.lang1, args.lang2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_np, test_np = train_test_split(langdata, test_size=0.15, random_state=1234)\n",
    "train, test = LangDataset(train_np), LangDataset(test_np)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "print(\"Read in %i train and %i test\" % (len(train), len(test)))\n",
    "\n",
    "# Initialize model\n",
    "logreg = SimpleLogreg(train.n_features)\n",
    "\n",
    "num_epochs = args.passes\n",
    "batch = args.batch\n",
    "total_samples = len(train)\n",
    "\n",
    "# Replace these with the correct loss and optimizer\n",
    "criterion = None\n",
    "optimizer = None\n",
    "\n",
    "train_loader = DataLoader(dataset=train,\n",
    "                          batch_size=batch,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "dataiter = iter(train_loader)\n",
    "\n",
    "# Iterations\n",
    "for epoch in range(num_epochs):\n",
    "  for ex, (inputs, labels) in enumerate(train_loader):\n",
    "    # Run your training process\n",
    "    step(epoch, ex, logreg, optimizer, criterion, inputs, labels)\n",
    "\n",
    "# Print out the best features\n",
    "vocab = read_vocab(open(args.vocab))\n",
    "logreg.inspect(vocab)"
   ],
   "metadata": {
    "id": "VEX0D68na2Q9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
