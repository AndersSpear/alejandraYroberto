{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pyshark\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import argparse\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T04:55:01.144707Z",
     "start_time": "2024-11-21T04:54:59.632029Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# sets up constants / arguments\n",
    "\n",
    "# argparser = argparse.ArgumentParser()\n",
    "# #''' Switch between the toy and REAL EXAMPLES\n",
    "# argparser.add_argument(\"--lang1\", help=\"Language 1 class\",\n",
    "#                        type=str, default=\"./data/englishpcaps\")\n",
    "# argparser.add_argument(\"--lang2\", help=\"Language 2 class\",\n",
    "#                        type=str, default=\"./data/spanishpcaps\")\n",
    "# argparser.add_argument(\"--passes\", help=\"Number of passes through train\",\n",
    "#                        type=int, default=5)\n",
    "# argparser.add_argument(\"--batch\", help=\"Number of items in each batch\",\n",
    "#                        type=int, default=1)\n",
    "# argparser.add_argument(\"--learnrate\", help=\"Learning rate for SGD\",\n",
    "#                        type=float, default=0.1)\n",
    "\n",
    "# args = argparser.parse_args()\n",
    "args = {\"lang1\":'./data/smallenglish', \"lang2\":'./data/smallspanish', \"passes\":5, \"batch\":1, \"learnrate\":0.1}\n",
    "\n",
    "\n",
    "vocab = [(0, 0, 0, 0), (0, 0, 0, 1), (0, 0, 0, 2), (0, 0, 1, 0), (0, 0, 1, 1), (0, 0, 1, 2),\n",
    " (0, 0, 2, 0), (0, 0, 2, 1), (0, 0, 2, 2), (0, 1, 0, 0), (0, 1, 0, 1), (0, 1, 0, 2),\n",
    " (0, 1, 1, 0), (0, 1, 1, 1), (0, 1, 1, 2), (0, 1, 2, 0), (0, 1, 2, 1), (0, 1, 2, 2),\n",
    " (0, 2, 0, 0), (0, 2, 0, 1), (0, 2, 0, 2), (0, 2, 1, 0), (0, 2, 1, 1), (0, 2, 1, 2),\n",
    " (0, 2, 2, 0), (0, 2, 2, 1), (0, 2, 2, 2), (1, 0, 0, 0), (1, 0, 0, 1), (1, 0, 0, 2),\n",
    " (1, 0, 1, 0), (1, 0, 1, 1), (1, 0, 1, 2), (1, 0, 2, 0), (1, 0, 2, 1), (1, 0, 2, 2),\n",
    " (1, 1, 0, 0), (1, 1, 0, 1), (1, 1, 0, 2), (1, 1, 1, 0), (1, 1, 1, 1), (1, 1, 1, 2),\n",
    " (1, 1, 2, 0), (1, 1, 2, 1), (1, 1, 2, 2), (1, 2, 0, 0), (1, 2, 0, 1), (1, 2, 0, 2),\n",
    " (1, 2, 1, 0), (1, 2, 1, 1), (1, 2, 1, 2), (1, 2, 2, 0), (1, 2, 2, 1), (1, 2, 2, 2),\n",
    " (2, 0, 0, 0), (2, 0, 0, 1), (2, 0, 0, 2), (2, 0, 1, 0), (2, 0, 1, 1), (2, 0, 1, 2),\n",
    " (2, 0, 2, 0), (2, 0, 2, 1), (2, 0, 2, 2), (2, 1, 0, 0), (2, 1, 0, 1), (2, 1, 0, 2),\n",
    " (2, 1, 1, 0), (2, 1, 1, 1), (2, 1, 1, 2), (2, 1, 2, 0), (2, 1, 2, 1), (2, 1, 2, 2),\n",
    " (2, 2, 0, 0), (2, 2, 0, 1), (2, 2, 0, 2), (2, 2, 1, 0), (2, 2, 1, 1), (2, 2, 1, 2),\n",
    " (2, 2, 2, 0), (2, 2, 2, 1), (2, 2, 2, 2)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T04:55:01.153595Z",
     "start_time": "2024-11-21T04:55:01.146150Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# define some helper functions\n",
    "def pcap_to_lengths(pcap_file) -> [int]:\n",
    "    length = []\n",
    "    capture = pyshark.FileCapture(pcap_file)\n",
    "    for packet in capture:\n",
    "        length.append(packet.length)\n",
    "\n",
    "    return length\n",
    "\n",
    "def lengths_to_tokens(lengths: [int]) -> [int]:\n",
    "    # 0 = 2 smallest\n",
    "    # 2 = largest length\n",
    "    # 1 = everything else\n",
    "\n",
    "    lengths_mod = lengths\n",
    "    lengths_mod.remove(min(lengths_mod))\n",
    "\n",
    "    min_length2 = min(lengths_mod) # second smallest\n",
    "    min_length = min(lengths)      # smallest\n",
    "    max_length = max(lengths)      # largest\n",
    "\n",
    "    for i in range(len(lengths)):\n",
    "        if lengths[i] == min_length or lengths[i] == min_length2:\n",
    "            lengths[i] = 0\n",
    "        elif lengths[i] == max_length:\n",
    "            lengths[i] = 2\n",
    "        else:\n",
    "            lengths[i] = 1\n",
    "\n",
    "    return lengths\n",
    "\n",
    "def tokens_to_tuples(tokens: [int]) -> [(int, int, int, int)]:\n",
    "    tuples = []\n",
    "    i = 0\n",
    "    while i + 3 < len(tokens):\n",
    "        tuples.append((tokens[i], tokens[i+1], tokens[i+2], tokens[i+3]))\n",
    "        i = i+1\n",
    "\n",
    "    return tuples\n",
    "\n",
    "def count_tuples(words: [(int, int, int, int)]) -> [int]:\n",
    "    # return type is {int: int} where key is index of tuple in vocab and value is the count\n",
    "    count = []\n",
    "    for i in range(len(vocab)):\n",
    "        count.append(words.count(vocab[i]))\n",
    "\n",
    "    return count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T04:55:01.159192Z",
     "start_time": "2024-11-21T04:55:01.154581Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# read in the pcap files, takes 10 seconds per file\n",
    "\n",
    "directory1 = Path(args[\"lang1\"])\n",
    "directory2 = Path(args[\"lang2\"])\n",
    "\n",
    "pcap_files1 = list(directory1.glob('*.pcapng'))\n",
    "pcap_files2 = list(directory2.glob('*.pcapng'))\n",
    "# convert pcaps to lengths\n",
    "lang1_lengths = [pcap_to_lengths(f) for f in pcap_files1]\n",
    "lang2_lengths = [pcap_to_lengths(f) for f in pcap_files2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T04:56:59.310416Z",
     "start_time": "2024-11-21T04:55:01.160079Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# process the pcaps, create the token count matrix ready to pass into the model\n",
    "\n",
    "# convert lengths to tokens\n",
    "lang1_tokens = [lengths_to_tokens(l) for l in lang1_lengths]\n",
    "lang2_tokens = [lengths_to_tokens(l) for l in lang2_lengths]\n",
    "\n",
    "# convert tokens to tuples\n",
    "lang1_tuples = [tokens_to_tuples(l) for l in lang1_tokens]\n",
    "lang2_tuples = [tokens_to_tuples(l) for l in lang2_tokens]\n",
    "\n",
    "# convert tuples to counts of each sample to insert into matrix -> [{int: int}]\n",
    "lang1_counts = [count_tuples(t) for t in lang1_tuples]\n",
    "lang2_counts = [count_tuples(t) for t in lang2_tuples]\n",
    "\n",
    "m1 = np.asmatrix(lang1_counts)\n",
    "n, _ = m1.shape\n",
    "zeros = np.zeros((n, 1))\n",
    "m1 = np.hstack((zeros, m1))\n",
    "\n",
    "m2 = np.asmatrix(lang2_counts)\n",
    "n, _ = m2.shape\n",
    "ones = np.ones((n, 1))\n",
    "m2 = np.hstack((ones, m2))\n",
    "\n",
    "langdata = np.vstack((m1, m2))\n",
    "\n",
    "assert langdata.shape == ((len(lang1_counts) + len(lang2_counts)), 1 + len(vocab))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T04:56:59.630350Z",
     "start_time": "2024-11-21T04:56:59.311375Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#write langdata to json\n",
    "import json\n",
    "matrix_list = langdata.tolist()\n",
    "\n",
    "# Write the matrix to a JSON file\n",
    "with open('data/fulldata.json', 'w') as f:\n",
    "    json.dump(matrix_list, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T05:04:21.996156Z",
     "start_time": "2024-11-21T05:02:45.137405Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 17 train and 3 test\n"
     ]
    }
   ],
   "source": [
    "# split the data into training and testing datasets\n",
    "# Define the LangDataset class to represent the data\n",
    "\n",
    "class LangDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.n_samples, self.n_features = data.shape\n",
    "        # The first column is label, the rest are the features\n",
    "        self.n_features -= 1\n",
    "\n",
    "        self.feature = torch.from_numpy(data[:, 1:].astype(np.float32)) # size [n_samples, n_features]\n",
    "        self.label = torch.from_numpy(data[:, [0]].astype(np.float32)) # size [n_samples, 1]\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.feature[index], self.label[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "train_np, test_np = train_test_split(langdata, test_size=0.15)\n",
    "train, test = LangDataset(train_np), LangDataset(test_np)\n",
    "print(\"Read in %i train and %i test\" % (len(train), len(test)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T04:56:59.639658Z",
     "start_time": "2024-11-21T04:56:59.631136Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# define the Logreg model and the step function\n",
    "\n",
    "class SimpleLogreg(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        \"\"\"\n",
    "        Initialize the parameters you'll need for the model.\n",
    "\n",
    "        :param num_features: The number of features in the linear model\n",
    "        \"\"\"\n",
    "        super(SimpleLogreg, self).__init__()\n",
    "        self.linear = nn.Linear(num_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the model prediction for an example.\n",
    "\n",
    "        :param x: Example to evaluate\n",
    "        \"\"\"\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "    def evaluate(self, data):\n",
    "        with torch.no_grad():\n",
    "            y_predicted = self(data.feature)\n",
    "            y_predicted_cls = y_predicted.round()\n",
    "            acc = y_predicted_cls.eq(data.label).sum() / float(data.label.shape[0])\n",
    "            return acc\n",
    "\n",
    "    def inspect(self, vocab, limit=10):\n",
    "        \"\"\"\n",
    "        A fundtion to find the top features and print them.\n",
    "        \"\"\"\n",
    "\n",
    "        None\n",
    "        weights = logreg.linear.weight[0].detach().numpy()\n",
    "\n",
    "def step(epoch, ex, model, optimizer, criterion, inputs, labels):\n",
    "    \"\"\"Take a single step of the optimizer, we factored it into a single\n",
    "    function so we could write tests.\n",
    "\n",
    "\n",
    "    :param epoch: The current epoch\n",
    "    :param ex: Which example / minibatch you're one\n",
    "    :param model: The model you're optimizing\n",
    "    :param inputs: The current set of inputs\n",
    "    :param labels: The labels for those inputs\n",
    "\n",
    "    A) get predictions\n",
    "    B) compute the loss from that prediction\n",
    "    C) backprop\n",
    "    D) update the parameters\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    prediction = model(inputs)\n",
    "    loss = criterion(prediction, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (ex+1) % 20 == 0:\n",
    "      acc_train = model.evaluate(train)\n",
    "      acc_test = model.evaluate(test)\n",
    "      print(f'Epoch: {epoch+1}/{num_epochs}, Example {ex}, loss = {loss.item():.4f}, train_acc = {acc_train.item():.4f} test_acc = {acc_test.item():.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T04:56:59.645Z",
     "start_time": "2024-11-21T04:56:59.640459Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "logreg = SimpleLogreg(train.n_features)\n",
    "num_epochs = args[\"passes\"]\n",
    "batch = args[\"batch\"]\n",
    "total_samples = len(train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T04:58:45.648998Z",
     "start_time": "2024-11-21T04:58:45.644802Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# initialize the loss function and the optimizer, then train the model\n",
    "\n",
    "# Replace these with the correct loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(logreg.parameters(), lr=args[\"learnrate\"])\n",
    "\n",
    "train_loader = DataLoader(dataset=train,\n",
    "                          batch_size=batch,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "dataiter = iter(train_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T04:58:50.586431Z",
     "start_time": "2024-11-21T04:58:50.053065Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Iterations (i think this is running the model?\n",
    "for epoch in range(num_epochs):\n",
    "  for ex, (inputs, labels) in enumerate(train_loader):\n",
    "    # Run your training process\n",
    "    step(epoch, ex, logreg, optimizer, criterion, inputs, labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T04:58:51.714504Z",
     "start_time": "2024-11-21T04:58:51.652343Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# i dont actually know what this model is doing or like if theres an easier way (there probably is)\n",
    "\n",
    "logreg.inspect(vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T05:00:00.741975Z",
     "start_time": "2024-11-21T05:00:00.739199Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
